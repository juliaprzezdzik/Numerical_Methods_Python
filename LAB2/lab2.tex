\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc} % Kodowanie UTF-8 umożliwia używanie polskich znaków
\usepackage[T1]{fontenc} % Obsługa fontów z kodyfikacją T1 dla poprawnego zapisu polskich znaków w PDF
\usepackage{tabularx} % Importuj pakiet tabularx do odpowiedniego rozmiaru tabeli
\usepackage{geometry} %np do zmieniania marginesow
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath} 
\usepackage{tocloft}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{hyperref}
\newgeometry{tmargin=2cm, bmargin=2cm, lmargin=2cm, rmargin=2cm}
\Large %wielkosc czcionki
\renewcommand{\contentsname}{Spis Treści}
\begin{document}
\Large
\begin{center}
{\LARGE \textbf{Sprawozdanie nr 3 z przedmiotu Metody Numeryczne}}
\\
{\large{Rozwiązywanie układów równań liniowych metodą największego spadku oraz metodą sprzężonych gradientów dla macierzy wstęgowej
}}
\\
{\large{Julia Przeździk}}
\\
\normalsize{16 marca 2024 r.}
\\
\end{center}
\large
\tableofcontents
\newpage
\section{Cel ćwiczenia}
Celem zadania było zapoznanie się oraz rozwiązanie zadania przy pomocy iteracyjnych metod numerycznych - metody największego spadku, a także metody sprzężonych gradientów. Należało zaimplementować odpowiednie funkcje rozwiązujące algebraiczny układ równań w wybranym języku programowania, a następnie utworzyć stosowne wykresy.
\section{Opis problemu}
Należało utworzyć macierz kwadratową A o wymiarze n = 1000 i przy założeniu, że m = 5 wypełnić jej elementy zgodnie z formułą:
\begin{equation*}
    A[i][j] = \frac{1}{1+|i-j|}, \quad gdy \quad|i-j| \leq{m}, \quad i,j = 0,..,n-1
\end{equation*}
\begin{equation*}
    A[i][j] = 0,  \quad gdy \quad |i-j| > m
\end{equation*}
Następnie należało zdefiniować wektor wyrazów wolnych:
\begin{equation*}
    b[i] = i, \quad i = 0,..,n-1
\end{equation*}
Wykorzystując te dane należało zaprogramować metodę największego spadku oraz metodę sprzężonych gradientów.
\section{Część teoretyczna}
\subsection{Macierz wstęgowa}
Macierzą wstęgową nazywamy macierz kwadratową, która posiada same elementy zerowe poza charakterystyczną wstęgą obejmującą diagonalę oraz przekątne nad i pod nią:
\[
\begin{bmatrix}
a_{11} & a_{12} & 0 & \dots & 0 \\
a_{21} & a_{22} & a_{23} & \dots & 0 \\
0 & a_{32} & a_{33} & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & a_{n-1,n} \\
0 & \dots & 0 & a_{n,n-1} & a_{nn}
\end{bmatrix}
\]
\subsection{Metoda największego spadku}
Metoda ta polega na wykonywaniu w każdej iteracji kroku w stronę wyznaczoną przez gradient w celu znalezienia minimum funkcji. Na początku należy wybrać punkt startowy $x_0$, w którym oblicza się antygradient, aby znaleźć kierunek poszukiwań w metodzie. Przybliżone rozwiązanie w i+1 iteracji wygląda następująco:
\begin{equation*}
    x_{i+1} = x_i + \alpha_iv_i
\end{equation*}
Jako $v_i$ należy wybrać kierunek gradientu $Q$:
\begin{equation*}
    \nabla Q = Ax_i - b = -r_i \implies v_i = -r_i
\end{equation*}
Aby znaleźć współczynnik $\alpha_i$, trzeba znaleźć wartość $Q(x_{i+1})$:
\begin{equation*}
    Q(x_i - \alpha_ir_i) = -\frac{1}{2}x_i^Tr - \frac{1}{2}x_i^Tb + \frac{1}{2}\alpha_i^2r_i^TAr_i + \alpha_ir_i^Tr_i
\end{equation*}
Aby znaleźć minimum, należy zróżniczkować po parametrze wariacyjnym:
\begin{equation*}
    \frac{\delta Q}{\delta \alpha_i} = r_i^Tr_i + \alpha_ir_i^TAr_i
\end{equation*}
\begin{equation*}
    \frac{\delta Q}{\delta \alpha_i} = 0 \implies \alpha_i = -\frac{r_i^Tr_i}{r_i^TAr_i}
\end{equation*}
Następnie wyznacza się kolejne przybliżenie, dla którego zachodzi warunek: 
\begin{equation*}
    Q(x_{i+1}) < Q(x_i) 
\end{equation*}
Przykładowa trajektoria prezentuje się następująco:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{Zrzut ekranu 2024-03-20 o 23.13.59.jpg}
    \caption*{\small{Źródło: \url{galaxy.agh.edu.pl/chwiej/mn/wyk/uarl_iter_22_23.pdf}}}
    \label{fig:enter-label}
\end{figure}
W implementacji metody należy skorzystać z następującego algorytmu: w pętli "do" powinny zawierać się równania:
\begin{gather*}
    r_k = b-Ax_k \\
    \alpha_k = \frac{r_k^Tr_k}{r_k^TAr_k}\\
    x_{k+1}=x_k+\alpha_kr_k
\end{gather*}
Gdy druga norma euklidesowa wektora reszt jest większa co do wartości od $10^{-6}$.
\subsection{Metoda sprzężonych gradientów}
Metoda ta pozwala na rozwiązanie układów równań liniowych, które można przedstawić za pomocą dodatnio określonej macierzy symetrycznej, czyli macierzy kwadratowej spełniającej odpowiednio warunki:
\begin{equation*}
    x^TAx > 0
\end{equation*}
gdzie x jest wektorem różnym od zera oraz
\begin{equation*}
    a_{ij} = a_{ji} \quad i,j = 1,2,...,n
\end{equation*}
Przy rozwiązywaniu układu równań tą metodą trzeba założyć, że $x_d$ jest rozwiązaniem dokładnym oraz ciąg wektorów ${y_1, y_2, y_3,...,y_n}$ to baza n-wymiarowej przestrzeni euklidesowej.
Różnicę rozwiązania dokładnego i przybliżonego można zapisać jako
\begin{equation*}
    \Delta x = x_d - x_y = \sum \alpha_j y_j
\end{equation*}
Jeśli elementy bazy są ortogonalne, można wyznaczyć współczynniki kombinacji liniowej i dzięki temu powstaje wzór:
\begin{equation*}
    \alpha_m = \frac{y_m^T(x_d-x_i)}{y_m^Ty_m} \quad i=1,2,..,n
\end{equation*}
Ponieważ wartość $x_d$ nie jest znana, należy obliczyć ponownie współczynnik $\alpha$ przy użyciu bazy A-ortogonalnej; efektywnie powstaje zależność:
\begin{equation*}
    \alpha_j = \frac{v_j^Tr_i}{v_j^TAv_j} \quad i,j=1,2,...,n
\end{equation*}
Wektory bazy powinny spełniać warunek A-ortogonalności, czyli
\begin{equation*}
    v_j^TAv_i \Longleftrightarrow i \neq j
\end{equation*}
Bazę A-ortogonalną można skonstruować za pomocą ortogonalizacji Grama-Schmidta, która polega na przekształceniu wektorów w wektory ortogonalne. \\
Kolejne przybliżenia wyznacza się zgodnie ze schematem:
\begin{gather*}
    v_1 = r_1 = b-Ax_1 \\
    \alpha_i = \frac{v_i^Tr_i}{v_i^TAv_i}\\
    x_{x+1} = x_i + \alpha_iv_i\\
    r_{i+1} = r_i - \alpha_iAv_i\\
    \beta_i = \frac{v_i^TAr_{i+1}}{v_i^TAv_i}\\
    v_{i+1} = r_{i+1}-\beta_iv_i
\end{gather*}
\newpage
\section{Wykorzystanie metod}
\subsection{Metoda największego spadku}
\subsubsection{Otrzymane wyniki}
Jako że całkowita liczba iteracji była dość dużą wartością, ponieważ wyniosła 129, w tabelach przedstawiono wartości dla 30 iteracji - 10 pierwszych iteracji, iteracji od k = 60 do k = 69 oraz 10 ostatnich iteracji. 
\begin{table}[htbp]
\centering
\caption*{\textbf{Tabela nr 1:} Wartości zmiennych dla metody największego spadku gdy $x_0 = 0$}
\label{tab:moja_tabela}
\begin{tabular}{cccc}
\toprule
Nr iteracji (k) & Norma euklidesowa reszty & $\alpha_k$ & Norma euklidesowa rozwiązań \\
\midrule
1 & 482.3034 & 0.25711 & 4690.69375 \\
2 & 211.97169 & 0.4969 & 4696.81207 \\
3 & 121.60652 & 0.45734 & 4709.05166 \\
4 & 82.21506 & 0.47743 & 4702.36197 \\
5 & 56.14969 & 0.45254 & 4712.57828 \\
6 & 42.09859 & 0.47684 & 4706.30473 \\
7 & 30.90504 & 0.45374 & 4713.47204 \\
8 & 24.44606 & 0.47888 & 4708.70773 \\
9 & 18.69388 & 0.45573 & 4713.66678 \\
10 & 15.26911 & 0.481 & 4710.19671 \\
\midrule
60 & 0.00663 & 0.49395 & 4713.17079 \\
61 & 0.00565 & 0.46837 & 4713.17288 \\
62 & 0.00508 & 0.49409 & 4713.17111 \\
63 & 0.00433 & 0.46848 & 4713.17272 \\
64 & 0.00389 & 0.49422 & 4713.17136 \\
65 & 0.00332 & 0.46859 & 4713.17259 \\
66 & 0.00299 & 0.49434 & 4713.17154 \\
67 & 0.00255 & 0.4687 & 4713.17249 \\
68 & 0.00229 & 0.49444 & 4713.17169 \\
69 & 0.00196 & 0.46879 & 4713.17241 \\
\midrule
120 & 0.000 & 0.49581 & 4713.17215 \\
121 & 0.000 & 0.46998 & 4713.17215 \\
122 & 0.000 & 0.49583 & 4713.17215 \\
123 & 0.000 & 0.470 & 4713.17215 \\
124 & 0.000 & 0.49585 & 4713.17215 \\
125 & 0.000 & 0.47002 & 4713.17215 \\
126 & 0.000 & 0.49588 & 4713.17215 \\
127 & 0.000 & 0.47004 & 4713.17215 \\
128 & 0.000 & 0.4959 & 4713.17215 \\
129 & 0.000 & 0.47006 & 4713.17215 \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[htbp]
\centering
\caption*{\textbf{Tabela nr 2:} Wartości zmiennych dla metody największego spadku gdy $x_1 = 1$}
\label{tab:moja_druga_tabela}
\begin{tabular}{cccc}
\toprule
Nr iteracji (k) & Norma euklidesowa reszty & $\alpha_k$ & Norma euklidesowa rozwiązań \\
\midrule
1 & 482.30340 & 0.25711 & 4690.69375 \\
2 & 211.97169 & 0.49690 & 4696.81207 \\
3 & 121.60652 & 0.45734 & 4709.05166 \\
4 & 82.21506 & 0.47743 & 4702.36197 \\
5 & 56.14969 & 0.45254 & 4712.57828 \\
6 & 42.09859 & 0.47684 & 4706.30473 \\
7 & 30.90504 & 0.45374 & 4713.47204 \\
8 & 24.44606 & 0.47888 & 4708.70773 \\
9 & 18.69388 & 0.45573 & 4713.66678 \\
10 & 15.26911 & 0.48100 & 4710.19671 \\
\midrule
60 & 0.00663 & 0.49395 & 4713.17079 \\
61 & 0.00565 & 0.46837 & 4713.17288 \\
62 & 0.00508 & 0.49409 & 4713.17111 \\
63 & 0.00433 & 0.46848 & 4713.17272 \\
64 & 0.00389 & 0.49422 & 4713.17136 \\
65 & 0.00332 & 0.46859 & 4713.17259 \\
66 & 0.00299 & 0.49434 & 4713.17154 \\
67 & 0.00255 & 0.46870 & 4713.17249 \\
68 & 0.00229 & 0.49444 & 4713.17169 \\
69 & 0.00196 & 0.46879 & 4713.17241 \\
\midrule
120 & 0.000 & 0.49581 & 4713.17215 \\
121 & 0.000 & 0.46998 & 4713.17215 \\
122 & 0.000 & 0.49583 & 4713.17215 \\
123 & 0.000 & 0.47000 & 4713.17215 \\
124 & 0.000 & 0.49585 & 4713.17215 \\
125 & 0.000 & 0.47002 & 4713.17215 \\
126 & 0.000 & 0.49588 & 4713.17215 \\
127 & 0.000 & 0.47004 & 4713.17215 \\
128 & 0.000 & 0.49590 & 4713.17215 \\
129 & 0.000 & 0.47006 & 4713.17215 \\
\bottomrule
\end{tabular}
\end{table}
\newpage
\subsubsection{Graficzne przedstawienie wyników}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Zrzut ekranu 2024-03-21 o 01.52.24.png}
    \caption*{\textbf{Wykres nr 1:} Norma wektora rozwiązań i norma wektora reszt w funkcji numeru iteracji k gdy $x_0 = 0$}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Zrzut ekranu 2024-03-21 o 01.57.22.png}
    \caption*{\textbf{Wykres nr 2:} Norma wektora rozwiązań i norma wektora reszt w funkcji numeru iteracji k gdy $x_1 = 1$}
    \label{fig:enter-label}
\end{figure}
\newpage
\subsection{Metoda sprzężonych gradientów}
\subsubsection{Otrzymane wyniki}
\begin{table}[htbp]
\centering
\caption*{\textbf{Tabela nr 1:} Wartości zmiennych dla metody sprzężonych gradientów}
\label{tab:metoda_sprzezonych_gradientow}
\begin{tabular}{cccc}
\toprule
Nr iteracji (k) & Norma euklidesowa reszty & $\alpha_k$ & Norma euklidesowa rozwiązań \\
\midrule
0 & 482.78548 & 0.25711 & 4697.72221 \\
1 & 211.02974 & 0.49757 & 4710.19934 \\
2 & 103.52254 & 0.56204 & 4715.98681 \\
3 & 54.41853 & 0.58658 & 4718.38808 \\
4 & 30.86507 & 0.61973 & 4719.39898 \\
5 & 18.31839 & 0.64731 & 4719.85421 \\
6 & 10.92065 & 0.65443 & 4720.06087 \\
7 & 6.45222 & 0.65005 & 4720.15011 \\
8 & 3.80599 & 0.64772 & 4720.18684 \\
9 & 2.26161 & 0.65008 & 4720.20165 \\
10 & 1.36224 & 0.65600 & 4720.20765 \\
11 & 0.83112 & 0.66325 & 4720.21011 \\
12 & 0.50833 & 0.66646 & 4720.21114 \\
13 & 0.30929 & 0.66416 & 4720.21156 \\
14 & 0.18823 & 0.66275 & 4720.21172 \\
15 & 0.11533 & 0.66585 & 4720.21179 \\
16 & 0.07076 & 0.66816 & 4720.21182 \\
17 & 0.04308 & 0.66517 & 4720.21183 \\
18 & 0.02604 & 0.66072 & 4720.21184 \\
19 & 0.01573 & 0.65944 & 4720.21184 \\
20 & 0.00951 & 0.66030 & 4720.21184 \\
21 & 0.00576 & 0.66067 & 4720.21184 \\
22 & 0.00348 & 0.66039 & 4720.21184 \\
23 & 0.00210 & 0.66020 & 4720.21184 \\
24 & 0.00127 & 0.66051 & 4720.21184 \\
25 & 0.00077 & 0.66194 & 4720.21184 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Graficzne przedstawienie wyników}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{Zrzut ekranu 2024-03-21 o 12.22.13.png}
    \begin{center}
        \textbf{Wykres nr 3}: Norma euklidesowa wektora reszt w funkcji numeru iteracji k\\
        \textbf{Wykres nr 4}: Norma euklidesowa wektora rozwiązań w funkcji numeru iteracji k
    \end{center}
    \label{fig:enter-label}
\end{figure}
\section{Wnioski}
Realizacja zadania umożliwiła zapoznanie się z iteracyjnymi metodami numerycznymi - metodą największego spadku oraz metodą sprzężonych gradientów, które zastosowano do rozwiązania algebraicznego układu równań wykorzystującego macierz wstęgową. Doświadczenie to pozwoliło na zrozumienie mechanizmów działania tych metod oraz ich praktycznego zastosowania w kontekście problemu numerycznego. 
\noindent
Obie metody wykazały się efektywnością w znajdowaniu rozwiązania układu równań, ponieważ oferują oszczędność zarówno pod względem obliczeniowym, jak i pamięciowym w porównaniu z tradycyjnymi metodami eliminacji.
\noindent
Realizacja zadania uwydatniła różnice między tymi metodami pod względem szybkości zbieżności do rozwiązania. Metoda sprzężonych gradientów, dzięki wykorzystaniu informacji z poprzednich iteracji do optymalizacji kierunku poszukiwań, często pozwala na szybsze osiągnięcie dokładnego rozwiązania w mniejszej liczbie iteracji.
\noindent
Ponadto, wybór odpowiedniej metody oraz jej parametrów startowych może mieć znaczący wpływ na efektywność rozwiązywania problemów numerycznych. W niektórych przypadkach, minimalne różnice w warunkach początkowych mogą prowadzić do znaczących różnic w liczbie wymaganych iteracji, co podkreśla potrzebę dokładnej analizy problemu przed wyborem metody rozwiązania.
\noindent
Wnioski te podkreślają znaczenie metod iteracyjnych w rozwiązywaniu układów równań liniowych, ale również wymagają rozumienia zarówno teoretycznych podstaw tych metod, jak i praktycznych aspektów ich stosowania.
\end{document}
